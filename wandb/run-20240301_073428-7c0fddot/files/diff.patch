diff --git a/main.py b/main.py
index 09d858c..6495890 100644
--- a/main.py
+++ b/main.py
@@ -32,6 +32,7 @@ def parse_args():
 	parser.add_argument('-ll','--layers_list',type=int, nargs = "+", help="list of number of layers")
 	# parser.add_argument('-rn','--run_name',type=str, help="name of the run")
 	parser.add_argument('-Mts','--Mt_list',type = int, nargs = "+", default = None, help = 'list of number of labeled target images per batch')
+	parser.add_argument('-dcms','--dimchange_multipliers',type = float, nargs = "+", default = None, help = 'list of dimchange multipliers. common for conv and linear layers.')
 	parser.add_argument('-beta','--beta',type = float,default = None, help = 'balance parameter between classfication and domain losses. beta =1 means zero contribution from domain loss.')
 	return parser.parse_args()
 
@@ -54,13 +55,14 @@ if __name__ == '__main__':
 
 
 	for layer in args.layers_list:
-		for Mt in args.Mt_list:
+		for dcm in args.dimchange_multipliers:
 			for ms in args.ms_list:
 				Ms = int(ms*batch_size)
+				Mt = 1
 				manual_seed = random.randint(1, 10000)
 				random.seed(manual_seed)
 				torch.manual_seed(manual_seed)
-
+            
 
 
 				# logging - (wandb)
@@ -77,8 +79,9 @@ if __name__ == '__main__':
 						# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
 						#"resume": 'allow',
 						#"allow_val_change": True,
-						"config":{"ms": ms, "Mt": Mt, "layer":layer, "beta":beta}
+						"config":{"ms": ms, "Mt": Mt, "layer":layer, "beta":beta, "dcm":dcm}
 						}
+				
 
 				# Logging setup (You can replace it with your preferred logging method)
 				wandb.init(**wandb_kwargs)
@@ -130,7 +133,7 @@ if __name__ == '__main__':
 
 				# load model
 
-				my_net = CNNModel(layer)
+				my_net = CNNModel(layer,dcm)
 
 				# setup optimizer
 
diff --git a/model.py b/model.py
index 4c65e91..8c67b06 100644
--- a/model.py
+++ b/model.py
@@ -4,20 +4,23 @@ from functions import ReverseLayerF
 
 class CNNModel(nn.Module):
 
-    def __init__(self,layers):
+    def __init__(self,layers, dimchange_multiplier):
         super(CNNModel, self).__init__()
+        dim_conv = int(64 * dimchange_multiplier)
+        dim_lin = int(100 * dimchange_multiplier)
+
         self.feature = nn.Sequential()
-        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))
-        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))
+        self.feature.add_module('f_conv1', nn.Conv2d(3, dim_conv, kernel_size=5))
+        self.feature.add_module('f_bn1', nn.BatchNorm2d(dim_conv))
         self.feature.add_module('f_pool1', nn.MaxPool2d(2))
         self.feature.add_module('f_relu1', nn.ReLU(True))
 
         for ii in range(layers-2):
-            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(64,64,padding = 2,kernel_size=5))
-            self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(64))
+            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(dim_conv,dim_conv,padding = 2,kernel_size=5))
+            self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(dim_conv))
             self.feature.add_module(f'f_relu{2+ii}', nn.ReLU(True))
  
-        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(64, 50, kernel_size=5))
+        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(dim_conv, 50, kernel_size=5))
         self.feature.add_module(f'f_bn{layers}', nn.BatchNorm2d(50))
         self.feature.add_module(f'f_drop{layers}', nn.Dropout2d())
         self.feature.add_module(f'f_pool{layers}', nn.MaxPool2d(2))
@@ -35,14 +38,14 @@ class CNNModel(nn.Module):
         self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))
 
         self.domain_classifier = nn.Sequential()
-        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, 100))
-        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))
+        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, dim_lin))
+        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(dim_lin))
         self.domain_classifier.add_module('d_relu1', nn.ReLU(True))
         for ii in range(layers-2): 
-            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(100, 100)) 
-            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(100))
+            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(dim_lin, dim_lin)) 
+            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(dim_lin))
             self.domain_classifier.add_module(f'd_relu{2+ii}', nn.ReLU(True))
-        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(100, 2))
+        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(dim_lin, 2))
         self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))
 
     def forward(self, input_data, alpha):
@@ -54,54 +57,3 @@ class CNNModel(nn.Module):
         domain_output = self.domain_classifier(reverse_feature)
 
         return class_output, domain_output
-
-class CNNModel_old(nn.Module):
-
-    def __init__(self,layers):
-        super(CNNModel, self).__init__()
-        self.feature = nn.Sequential()
-        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))
-        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))
-        self.feature.add_module('f_pool1', nn.MaxPool2d(2))
-        self.feature.add_module('f_relu1', nn.ReLU(True))
-        for ii in range(layers-2):
-            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(64,64,padding = 2,kernel_size=5))
-            self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(64))
-            self.feature.add_module(f'f_relu{2+ii}', nn.ReLU(True))
-        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(64, 50, kernel_size=5))
-        self.feature.add_module(f'f_bn{layers}', nn.BatchNorm2d(50))
-        self.feature.add_module(f'f_drop{layers}', nn.Dropout2d())
-        self.feature.add_module(f'f_pool{layers}', nn.MaxPool2d(2))
-        self.feature.add_module(f'f_relu{layers}', nn.ReLU(True))
-
-        self.class_classifier = nn.Sequential()
-        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 5 * 5, 100))
-        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))
-        self.class_classifier.add_module('c_relu1', nn.ReLU(True))
-        self.class_classifier.add_module('c_drop1', nn.Dropout())
-        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))
-        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))
-        self.class_classifier.add_module('c_relu2', nn.ReLU(True))
-        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))
-        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))
-
-        self.domain_classifier = nn.Sequential()
-        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 5 * 5, 100)) # bak buralar değişiyor 50*4*4 ten 50*5*5 oluyorlar.
-        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))
-        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))
-        for ii in range(layers-2): 
-            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(100, 100)) 
-            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(100))
-            self.domain_classifier.add_module(f'd_relu{2+ii}', nn.ReLU(True))
-        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(100, 2))
-        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))
-
-    def forward(self, input_data, gamma, ms_or_mt):
-        bs = input_data.data.shape[0]
-        Ms_or_Mt = int(bs*ms_or_mt)
-
-        input_data = input_data.expand(bs, 3, 32, 32)
-        feature = self.feature(input_data)
-        feature = feature.view(-1, 50 * 5 * 5) # bak burada da var bir değişikliki
-        reverse_feature = ReverseLayerF.apply(feature, gamma)
-        class_output = self.class_classifier(feature[0:Ms_or_Mt,:])
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 51f95ac..cef338c 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240209_165914-h2i2ay0n/logs/debug-internal.log
\ No newline at end of file
+run-20240301_073428-7c0fddot/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 73290ea..fd196e9 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240209_165914-h2i2ay0n/logs/debug.log
\ No newline at end of file
+run-20240301_073428-7c0fddot/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 734054b..0002aeb 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240209_165914-h2i2ay0n
\ No newline at end of file
+run-20240301_073428-7c0fddot
\ No newline at end of file
