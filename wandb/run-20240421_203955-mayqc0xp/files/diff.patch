diff --git a/main.py b/main.py
index 41976e7..7e3440c 100644
--- a/main.py
+++ b/main.py
@@ -34,6 +34,7 @@ def parse_args():
 	# parser.add_argument('-log','--log_wandb',type=bool, default= True, help="whether to log to wandb or not")
 	# parser.add_argument('-l','--layers',type=int, help="number of layers")
 	parser.add_argument('-ll','--layers_list',type=int, nargs = "+", help="list of number of layers")
+	parser.add_argument('-NsNt','--NsNt',type=int, nargs = "+", help="list of number of total source and target images")
 	# parser.add_argument('-rn','--run_name',type=str, help="name of the run")
 	# parser.add_argument('-Mts','--Mt_list',type = int, nargs = "+", default = None, help = 'list of number of labeled target images per batch')
 	parser.add_argument('-dcms','--dimchange_multipliers',type = float, nargs = "+", default = None, help = 'list of dimchange multipliers. common for conv and linear layers.')
@@ -59,264 +60,265 @@ if __name__ == '__main__':
 	image_size = 28
 	
 	beta = args.beta 
-	Ns = args.Ns
+	
 	Mt = args.Mt
-	Nt = args.Nt 
-
 
 	for repeat in range(args.repeats):
-		for layer in args.layers_list:
-			n_epoch = args.n_epoch if not args.adaptive_epochs else int(50*layer)
-			for dcm in args.dimchange_multipliers:
-				for Ms in args.Ms_list:
-					# Ms = int(ms*batch_size)
-					manual_seed = random.randint(1, 10000)
-					random.seed(manual_seed)
-					torch.manual_seed(manual_seed)
-            
-
-
-					# logging - (wandb)
-	
-					# run_name = get_run_name() if not args.run_name else args.run_name
-					# run_folder = '/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/runs/' + run_name
-					# os.makedirs(run_folder, exist_ok=True)
-
-
-					wandb_kwargs = {# "dir": run_folder,
-							# "name": run_name,
-							"project": args.project_name,
-							"notes": args.notes,
-							# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
-							#"resume": 'allow',
-							#"allow_val_change": True,
-							"config":{"Ms": Ms, 
-				 					  "Mt": Mt, 
-									  "layer":layer, 
-									  "beta":beta, 
-									  "dcm":dcm, 
-									  "Ns":Ns, 
-									  "Nt":Nt,
-									  "bs":batch_size,
-									  "repeat":repeat+1}
-							}
-					
-
-					# Logging setup (You can replace it with your preferred logging method)
-					wandb.init(**wandb_kwargs)
-					wandb.run.log_code('.')
-
-
-					# load data
-
-					## define transformations
-					img_transform_source = transforms.Compose([
-						transforms.Resize(image_size),
-						transforms.ToTensor(),
-						transforms.Normalize(mean=(0.1307,), std=(0.3081,))
-					])
-
-					img_transform_target = transforms.Compose([
-						transforms.Resize(image_size),
-						transforms.ToTensor(),
-						transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
-					])
-
-					## read source and target datasets for the first time:
-					dataset_source = datasets.MNIST(
-						root='dataset',
-						train=True,
-						transform=img_transform_source,
-						download=True
-					)
-
-					dataset_target = GetLoader(
-						data_root=os.path.join(target_image_root, 'mnist_m_train'),
-						data_list=os.path.join(target_image_root, 'mnist_m_train_labels.txt'),
-						transform=img_transform_target
-					)
-
-					## create subsets of the original source and target datasets. choose random Ns and Nt indices respectively:
-					dataset_source = Subset(dataset_source, indices=random.sample(range(len(dataset_source)),Ns))
-					dataset_target = Subset(dataset_target, indices=random.sample(range(len(dataset_target)),Nt))
-
-					## split labeled and unlabeled datasets:
-					dataset_source_labeled, dataset_source_unlabeled = random_split(dataset_source, [Ms, Ns-Ms])
-					dataset_target_labeled, dataset_target_unlabeled = random_split(dataset_target, [Mt, Nt-Mt])
-
-
-					## create labeled and unlabeled dataloaders seperately:
-					## (in order to prevent using more labeled total data than specified Ms and Mt values)
-
-					### source dataloaders
-					dataloader_source_labeled = DataLoader(
-						dataset=dataset_source_labeled,
-						batch_size=1,
-						shuffle=True,
-						num_workers=8,
-						drop_last = True)
+		for NsNt in args.NsNt:
+			Nt = NsNt
+			Ns = NsNt
+			for layer in args.layers_list:
+				n_epoch = args.n_epoch if not args.adaptive_epochs else int(50*layer)
+				for dcm in args.dimchange_multipliers:
+					for Ms in args.Ms_list:
+						# Ms = int(ms*batch_size)
+						manual_seed = random.randint(1, 10000)
+						random.seed(manual_seed)
+						torch.manual_seed(manual_seed)
+				
+
+
+						# logging - (wandb)
+		
+						# run_name = get_run_name() if not args.run_name else args.run_name
+						# run_folder = '/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/runs/' + run_name
+						# os.makedirs(run_folder, exist_ok=True)
+
+
+						wandb_kwargs = {# "dir": run_folder,
+								# "name": run_name,
+								"project": args.project_name,
+								"notes": args.notes,
+								# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
+								#"resume": 'allow',
+								#"allow_val_change": True,
+								"config":{"Ms": Ms, 
+										"Mt": Mt, 
+										"layer":layer, 
+										"beta":beta, 
+										"dcm":dcm, 
+										"Ns":Ns, 
+										"Nt":Nt,
+										"bs":batch_size,
+										"repeat":repeat+1}
+								}
 						
-					dataloader_source_unlabeled = DataLoader(
-						dataset=dataset_source_unlabeled,
-						batch_size=batch_size,
-						shuffle=True,
-						num_workers=8,
-						drop_last = True)
-
-					### target dataloaders
-					dataloader_target_labeled = DataLoader(
-						dataset=dataset_target_labeled,
-						batch_size=1,
-						shuffle=True,
-						num_workers=8,
-						drop_last = True)
-						
-					dataloader_target_unlabeled = DataLoader(
-						dataset=dataset_target_unlabeled,
-						batch_size=batch_size,
-						shuffle=True,
-						num_workers=8,
-						drop_last = True)
-
-					# load model
-	
-					my_net = CNNModel(layer,dcm,"mchange")
-
-					# setup optimizer
-
-					optimizer = optim.Adam(my_net.parameters(), lr=lr)
-
-					loss_class = torch.nn.NLLLoss(reduction = "sum")
-					loss_domain = torch.nn.NLLLoss(reduction = "sum")
-
-					if cuda:
-						my_net = my_net.cuda()
-						loss_class = loss_class.cuda()
-						loss_domain = loss_domain.cuda()
-
-					for p in my_net.parameters():
-						p.requires_grad = True
-					
 
-					# training
-					best_accu_t = 0.0
-					for epoch in range(n_epoch):
+						# Logging setup (You can replace it with your preferred logging method)
+						wandb.init(**wandb_kwargs)
+						wandb.run.log_code('.')
+
+
+						# load data
+
+						## define transformations
+						img_transform_source = transforms.Compose([
+							transforms.Resize(image_size),
+							transforms.ToTensor(),
+							transforms.Normalize(mean=(0.1307,), std=(0.3081,))
+						])
+
+						img_transform_target = transforms.Compose([
+							transforms.Resize(image_size),
+							transforms.ToTensor(),
+							transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
+						])
+
+						## read source and target datasets for the first time:
+						dataset_source = datasets.MNIST(
+							root='dataset',
+							train=True,
+							transform=img_transform_source,
+							download=True
+						)
+
+						dataset_target = GetLoader(
+							data_root=os.path.join(target_image_root, 'mnist_m_train'),
+							data_list=os.path.join(target_image_root, 'mnist_m_train_labels.txt'),
+							transform=img_transform_target
+						)
+
+						## create subsets of the original source and target datasets. choose random Ns and Nt indices respectively:
+						dataset_source = Subset(dataset_source, indices=random.sample(range(len(dataset_source)),Ns))
+						dataset_target = Subset(dataset_target, indices=random.sample(range(len(dataset_target)),Nt))
+
+						## split labeled and unlabeled datasets:
+						dataset_source_labeled, dataset_source_unlabeled = random_split(dataset_source, [Ms, Ns-Ms])
+						dataset_target_labeled, dataset_target_unlabeled = random_split(dataset_target, [Mt, Nt-Mt])
+
+
+						## create labeled and unlabeled dataloaders seperately:
+						## (in order to prevent using more labeled total data than specified Ms and Mt values)
+
+						### source dataloaders
+						dataloader_source_labeled = DataLoader(
+							dataset=dataset_source_labeled,
+							batch_size=1,
+							shuffle=True,
+							num_workers=8,
+							drop_last = True)
+							
+						dataloader_source_unlabeled = DataLoader(
+							dataset=dataset_source_unlabeled,
+							batch_size=batch_size,
+							shuffle=True,
+							num_workers=8,
+							drop_last = True)
+
+						### target dataloaders
+						dataloader_target_labeled = DataLoader(
+							dataset=dataset_target_labeled,
+							batch_size=1,
+							shuffle=True,
+							num_workers=8,
+							drop_last = True)
+							
+						dataloader_target_unlabeled = DataLoader(
+							dataset=dataset_target_unlabeled,
+							batch_size=batch_size,
+							shuffle=True,
+							num_workers=8,
+							drop_last = True)
 
-						# create iterators:
-						source_unlabeled_iter = iter(dataloader_source_unlabeled)
-						source_labeled_iter = iter(dataloader_source_labeled)
-						target_unlabeled_iter = iter(dataloader_target_unlabeled)
-						target_labeled_iter = iter(dataloader_target_labeled)
+						# load model
+		
+						my_net = CNNModel(layer,dcm,"mchange")
 
-						Ks = len(source_unlabeled_iter)
-						Kt = len(target_unlabeled_iter)
-						K = min(Ks,Kt)
+						# setup optimizer
 
-						### distribution of number of labeled samples per batch
-						Msx_list = distribute_apples(Ms,Ks)
-						Mtx_list = distribute_apples(Mt,Kt)										
+						optimizer = optim.Adam(my_net.parameters(), lr=lr)
 
-						for i in range(K): 
-							
-							# required number of labeled data for i'th batch to ensure the total of Ms or Mt 
-							Msx = Msx_list[i] 
-							Mtx = Mtx_list[i]
+						loss_class = torch.nn.NLLLoss(reduction = "sum")
+						loss_domain = torch.nn.NLLLoss(reduction = "sum")
 
-							p = float(i + epoch * K) / n_epoch / K
-							alpha = 2. / (1. + np.exp(-10 * p)) - 1
+						if cuda:
+							my_net = my_net.cuda()
+							loss_class = loss_class.cuda()
+							loss_domain = loss_domain.cuda()
 
-							my_net.zero_grad()
+						for p in my_net.parameters():
+							p.requires_grad = True
+						
 
-							# training model using source data
-							
-							source_unlabeled_img, _ = source_unlabeled_iter.next()
-							if Msx:
-								source_labeled_img, source_labeled_label  = source_labeled_iter.next()
-								for i in range(Msx-1):
-									source_labeled_img_temp, source_labeled_label_temp  = source_labeled_iter.next()
-									source_labeled_img = torch.cat((source_labeled_img,source_labeled_img_temp),dim = 0)
-									source_labeled_label = torch.cat((source_labeled_label,source_labeled_label_temp),dim = 0)
-
-								s_img = torch.cat((source_unlabeled_img[:-Msx,:,:,:],source_labeled_img[:Msx,:,:,:]),dim = 0)
-								s_label = source_labeled_label
-							else:
-								s_img = source_unlabeled_img
-								s_label = torch.rand([0]).long()
-							
-							
+						# training
+						best_accu_t = 0.0
+						for epoch in range(n_epoch):
+
+							# create iterators:
+							source_unlabeled_iter = iter(dataloader_source_unlabeled)
+							source_labeled_iter = iter(dataloader_source_labeled)
+							target_unlabeled_iter = iter(dataloader_target_unlabeled)
+							target_labeled_iter = iter(dataloader_target_labeled)
+
+							Ks = len(source_unlabeled_iter)
+							Kt = len(target_unlabeled_iter)
+							K = min(Ks,Kt)
+
+							### distribution of number of labeled samples per batch
+							Msx_list = distribute_apples(Ms,Ks)
+							Mtx_list = distribute_apples(Mt,Kt)										
+
+							for i in range(K): 
+								
+								# required number of labeled data for i'th batch to ensure the total of Ms or Mt 
+								Msx = Msx_list[i] 
+								Mtx = Mtx_list[i]
+
+								p = float(i + epoch * K) / n_epoch / K
+								alpha = 2. / (1. + np.exp(-10 * p)) - 1
+
+								my_net.zero_grad()
+
+								# training model using source data
+								
+								source_unlabeled_img, _ = source_unlabeled_iter.next()
+								if Msx:
+									source_labeled_img, source_labeled_label  = source_labeled_iter.next()
+									for i in range(Msx-1):
+										source_labeled_img_temp, source_labeled_label_temp  = source_labeled_iter.next()
+										source_labeled_img = torch.cat((source_labeled_img,source_labeled_img_temp),dim = 0)
+										source_labeled_label = torch.cat((source_labeled_label,source_labeled_label_temp),dim = 0)
+
+									s_img = torch.cat((source_unlabeled_img[:-Msx,:,:,:],source_labeled_img[:Msx,:,:,:]),dim = 0)
+									s_label = source_labeled_label
+								else:
+									s_img = source_unlabeled_img
+									s_label = torch.rand([0]).long()
+								
+								
+
+								domain_label = torch.zeros(batch_size).long()
+								
+								# batch_size = len(s_label)
+
+								if cuda:
+									s_img = s_img.cuda()
+									s_label = s_label.cuda()
+									domain_label = domain_label.cuda()
+
+
+								class_output, domain_output = my_net(input_data=s_img, alpha=alpha)
+								err_s_label = loss_class(class_output[(batch_size-Msx):,:], s_label)
+								err_s_domain = loss_domain(domain_output, domain_label)
+
+								# training model using target data
+								target_unlabeled_img, _ = target_unlabeled_iter.next()
+								if Mtx:
+									target_labeled_img, target_labeled_label  = target_labeled_iter.next()
+									for i in range(Mtx-1):
+										target_labeled_img_temp, target_labeled_label_temp  = target_labeled_iter.next()
+										target_labeled_img = torch.cat((target_labeled_img,target_labeled_img_temp),dim = 0)
+										target_labeled_label = torch.cat((target_labeled_label,target_labeled_label_temp),dim = 0)
+
+									t_img = torch.cat((target_unlabeled_img[:-Mtx,:,:,:],target_labeled_img[:Mtx,:,:,:]),dim = 0)
+									t_label = target_labeled_label
+								else:
+									t_img = target_unlabeled_img
+									t_label = torch.rand([0]).long()
+
+								# batch_size = len(t_img)
+
+								domain_label = torch.ones(batch_size).long()
+
+								if cuda:
+									t_img = t_img.cuda()
+									t_label = t_label.cuda()
+									domain_label = domain_label.cuda()
+
+								class_output, domain_output = my_net(input_data=t_img, alpha=alpha)
+								err_t_label = loss_class(class_output[(batch_size-Mtx):,:], t_label)
+								err_t_domain = loss_domain(domain_output, domain_label)
+
+								err =(1-beta)*(err_t_domain + err_s_domain) + beta*(err_s_label + err_t_label)
+								err.backward()
+								optimizer.step()
+
+								# sys.stdout.write('\r epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \
+								#       % (epoch, i + 1, len_dataloader, err_s_label.data.cpu().numpy(),
+								#          err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()))
+								# sys.stdout.flush()
+								torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))
+
+							print('\n')
+							print('Ms: %d | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (Ms,Mt,dcm,layer,epoch+1,n_epoch))
+							accu_s = test(source_dataset_name)
+							print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))
+							accu_t = test(target_dataset_name)
+							print('Accuracy of the %s dataset: %f\n' % ('mnist_m', accu_t))
+
+							if accu_t > best_accu_t:
+								best_accu_s = accu_s
+								best_accu_t = accu_t
+								torch.save(my_net, '{0}/mnist_mnistm_model_epoch_best.pth'.format(model_root))
+
+							wandb.log({"err_s_label_train": err_s_label/Msx,
+							"err_s_domain_train": err_s_domain/batch_size,
+							"err_t_label_train": err_t_label/Mtx,
+							"err_t_domain_train": err_t_domain/batch_size,
+							"total_loss":err,
+							"accu_s_test": accu_s,
+							"accu_t_test": accu_t}, step=epoch)
 
-							domain_label = torch.zeros(batch_size).long()
-							
-							# batch_size = len(s_label)
-
-							if cuda:
-								s_img = s_img.cuda()
-								s_label = s_label.cuda()
-								domain_label = domain_label.cuda()
-
-
-							class_output, domain_output = my_net(input_data=s_img, alpha=alpha)
-							err_s_label = loss_class(class_output[(batch_size-Msx):,:], s_label)
-							err_s_domain = loss_domain(domain_output, domain_label)
-
-							# training model using target data
-							target_unlabeled_img, _ = target_unlabeled_iter.next()
-							if Mtx:
-								target_labeled_img, target_labeled_label  = target_labeled_iter.next()
-								for i in range(Mtx-1):
-									target_labeled_img_temp, target_labeled_label_temp  = target_labeled_iter.next()
-									target_labeled_img = torch.cat((target_labeled_img,target_labeled_img_temp),dim = 0)
-									target_labeled_label = torch.cat((target_labeled_label,target_labeled_label_temp),dim = 0)
-
-								t_img = torch.cat((target_unlabeled_img[:-Mtx,:,:,:],target_labeled_img[:Mtx,:,:,:]),dim = 0)
-								t_label = target_labeled_label
-							else:
-								t_img = target_unlabeled_img
-								t_label = torch.rand([0]).long()
-
-							# batch_size = len(t_img)
-
-							domain_label = torch.ones(batch_size).long()
-
-							if cuda:
-								t_img = t_img.cuda()
-								t_label = t_label.cuda()
-								domain_label = domain_label.cuda()
-
-							class_output, domain_output = my_net(input_data=t_img, alpha=alpha)
-							err_t_label = loss_class(class_output[(batch_size-Mtx):,:], t_label)
-							err_t_domain = loss_domain(domain_output, domain_label)
-
-							err =(1-beta)*(err_t_domain + err_s_domain) + beta*(err_s_label + err_t_label)
-							err.backward()
-							optimizer.step()
-
-							# sys.stdout.write('\r epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \
-							#       % (epoch, i + 1, len_dataloader, err_s_label.data.cpu().numpy(),
-							#          err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()))
-							# sys.stdout.flush()
-							torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))
-
-						print('\n')
-						print('Ms: %d | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (Ms,Mt,dcm,layer,epoch+1,n_epoch))
-						accu_s = test(source_dataset_name)
-						print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))
-						accu_t = test(target_dataset_name)
-						print('Accuracy of the %s dataset: %f\n' % ('mnist_m', accu_t))
-
-						if accu_t > best_accu_t:
-							best_accu_s = accu_s
-							best_accu_t = accu_t
-							torch.save(my_net, '{0}/mnist_mnistm_model_epoch_best.pth'.format(model_root))
-
-						wandb.log({"err_s_label_train": err_s_label/Msx,
-						"err_s_domain_train": err_s_domain/batch_size,
-						"err_t_label_train": err_t_label/Mtx,
-						"err_t_domain_train": err_t_domain/batch_size,
-						"total_loss":err,
-						"accu_s_test": accu_s,
-						"accu_t_test": accu_t}, step=epoch)
-
-					
-					wandb.finish()
-					
+						
+						wandb.finish()
+						
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index d75781d..a401f92 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240420_030248-f2h94kgb/logs/debug-internal.log
\ No newline at end of file
+run-20240421_203955-mayqc0xp/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index d6c878b..4c1bf37 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240420_030248-f2h94kgb/logs/debug.log
\ No newline at end of file
+run-20240421_203955-mayqc0xp/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 9a78731..62506bf 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240420_030248-f2h94kgb
\ No newline at end of file
+run-20240421_203955-mayqc0xp
\ No newline at end of file
