diff --git a/.vscode/launch.json b/.vscode/launch.json
index f1e8bd5..135a182 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -19,10 +19,13 @@
             "cwd": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3",
             "program": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/main.py",
             "args": ["-bs", "128", 
-                     "-ll","2",
-                     "-dcms","5", "4",
-                     "-Mss","40",
+                     "-ll","5",
+                     "-dcms","3",
+                     "-Mss","20","30","40",
+                     "-Mt","10",
                      "-ns","0.1",
+                     "-r","2",
+                     "-e","300",
                      "-n","debugging"
                     ],
             "console": "integratedTerminal",
diff --git a/main.py b/main.py
index bf3e27b..55cf872 100644
--- a/main.py
+++ b/main.py
@@ -59,7 +59,7 @@ if __name__ == '__main__':
 	Mt = args.Mt 
 
 
-	for _ in range(args.repeats):
+	for repeat in range(args.repeats):
 		for layer in args.layers_list:
 			for dcm in args.dimchange_multipliers:
 				for Ms in args.Ms_list:
@@ -84,7 +84,14 @@ if __name__ == '__main__':
 							# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
 							#"resume": 'allow',
 							#"allow_val_change": True,
-							"config":{"Ms": Ms, "Mt": Mt, "layer":layer, "beta":beta, "dcm":dcm, "ns":ns, "bs":batch_size}
+							"config":{"Ms": Ms, 
+				 					  "Mt": Mt, 
+									  "layer":layer, 
+									  "beta":beta, 
+									  "dcm":dcm, 
+									  "ns":ns, 
+									  "bs":batch_size,
+									  "repeat":repeat+1}
 							}
 					
 
@@ -138,7 +145,7 @@ if __name__ == '__main__':
 
 					# load model
 	
-					my_net = CNNModel(layer,dcm)
+					my_net = CNNModel(layer,dcm,"mchange")
 
 					# setup optimizer
 
@@ -223,7 +230,7 @@ if __name__ == '__main__':
 							torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))
 
 						print('\n')
-						print('Ms: %d | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (Ms,Mt,dcm,layer,epoch,n_epoch))
+						print('Ms: %d | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (Ms,Mt,dcm,layer,epoch+1,n_epoch))
 						accu_s = test(source_dataset_name)
 						print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))
 						accu_t = test(target_dataset_name)
diff --git a/model.py b/model.py
index b295f10..2363d2b 100644
--- a/model.py
+++ b/model.py
@@ -5,48 +5,66 @@ from functions import ReverseLayerF
 
 class CNNModel(nn.Module):
 
-    def __init__(self,layers, dimchange_multiplier):
+    def __init__(self,layers, dimchange_multiplier,m_or_n_change):
         super(CNNModel, self).__init__()
-        dim_conv = int(64 * dimchange_multiplier)
-        dim_lin = int(100 * dimchange_multiplier)
+
+        #Â dim_conv = int(64 * dimchange_multiplier)
+        dim_conv = 64
+        ks = int( 5 *dimchange_multiplier)
+        padding_conv1 = int((ks-5)/2)
+        padding_convs = int((ks-1)/2)
+        layers_feature = layers
+        if m_or_n_change == "mchange":
+            dim_lin_class = int(100 * dimchange_multiplier)
+            layers_class = layers
+
+            dim_lin_domain = 100
+            layers_domain = 2
+
+        if m_or_n_change == "nchange":
+            dim_lin_class = 100
+            layers_class = 3
+
+            dim_lin_domain = int(100 * dimchange_multiplier)
+            layers_domain = layers            
+        
 
         self.feature = nn.Sequential()
-        self.feature.add_module('f_conv1', nn.Conv2d(3, dim_conv, kernel_size=5))
+        self.feature.add_module('f_conv1', nn.Conv2d(3, dim_conv, kernel_size=ks,padding=padding_conv1))
         self.feature.add_module('f_bn1', nn.BatchNorm2d(dim_conv))
         self.feature.add_module('f_pool1', nn.MaxPool2d(2))
         self.feature.add_module('f_relu1', nn.ReLU(True))
-
-        for ii in range(layers-2):
-            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(dim_conv,dim_conv,padding = 2,kernel_size=5))
+        for ii in range(layers_feature-2):
+            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(dim_conv,dim_conv,padding = padding_convs,kernel_size=ks))
             self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(dim_conv))
             self.feature.add_module(f'f_relu{2+ii}', nn.ReLU(True))
- 
-        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(dim_conv, 50, kernel_size=5))
-        self.feature.add_module(f'f_bn{layers}', nn.BatchNorm2d(50))
-        self.feature.add_module(f'f_drop{layers}', nn.Dropout2d())
-        self.feature.add_module(f'f_pool{layers}', nn.MaxPool2d(2))
-        self.feature.add_module(f'f_relu{layers}', nn.ReLU(True))
+        self.feature.add_module(f'f_conv{layers_feature}', nn.Conv2d(dim_conv, 50, kernel_size=ks,padding=padding_conv1))
+        self.feature.add_module(f'f_bn{layers_feature}', nn.BatchNorm2d(50))
+        self.feature.add_module(f'f_drop{layers_feature}', nn.Dropout2d())
+        self.feature.add_module(f'f_pool{layers_feature}', nn.MaxPool2d(2))
+        self.feature.add_module(f'f_relu{layers_feature}', nn.ReLU(True))
 
         self.class_classifier = nn.Sequential()
-        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 4 * 4, 100))
-        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))
+        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 4 * 4, dim_lin_class))
+        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(dim_lin_class))
         self.class_classifier.add_module('c_relu1', nn.ReLU(True))
         self.class_classifier.add_module('c_drop1', nn.Dropout())
-        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))
-        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))
-        self.class_classifier.add_module('c_relu2', nn.ReLU(True))
-        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))
+        for ii in range(layers_class-2):
+            self.class_classifier.add_module(f'c_fc{2+ii}', nn.Linear(dim_lin_class, dim_lin_class))
+            self.class_classifier.add_module(f'c_bn{2+ii}', nn.BatchNorm1d(dim_lin_class))
+            self.class_classifier.add_module(f'c_relu{2+ii}', nn.ReLU(True))
+        self.class_classifier.add_module(f'c_fc{layers_class}', nn.Linear(dim_lin_class, 10))
         self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))
 
         self.domain_classifier = nn.Sequential()
-        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, dim_lin))
-        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(dim_lin))
+        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, dim_lin_domain))
+        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(dim_lin_domain))
         self.domain_classifier.add_module('d_relu1', nn.ReLU(True))
-        for ii in range(layers-2): 
-            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(dim_lin, dim_lin)) 
-            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(dim_lin))
+        for ii in range(layers_domain-2): 
+            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(dim_lin_domain, dim_lin_domain)) 
+            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(dim_lin_domain))
             self.domain_classifier.add_module(f'd_relu{2+ii}', nn.ReLU(True))
-        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(dim_lin, 2))
+        self.domain_classifier.add_module(f'd_fc{layers_domain}', nn.Linear(dim_lin_domain, 2))
         self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))
 
     def forward(self, input_data, alpha):
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index fdd0702..548f15c 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240313_131454-3g4vkahr/logs/debug-internal.log
\ No newline at end of file
+run-20240316_194820-dlzfdccw/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index e4a10b6..a96f21e 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240313_131454-3g4vkahr/logs/debug.log
\ No newline at end of file
+run-20240316_194820-dlzfdccw/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index c6ac0ac..f8db9af 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240313_131454-3g4vkahr
\ No newline at end of file
+run-20240316_194820-dlzfdccw
\ No newline at end of file
