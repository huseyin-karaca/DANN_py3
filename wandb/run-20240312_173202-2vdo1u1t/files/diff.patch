diff --git a/.vscode/launch.json b/.vscode/launch.json
index 289c379..f1e8bd5 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -18,11 +18,12 @@
             "request": "launch",
             "cwd": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3",
             "program": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/main.py",
-            "args": [// "-bs", "512", 
+            "args": ["-bs", "128", 
                      "-ll","2",
                      "-dcms","5", "4",
-                     "-mss","0.1",
+                     "-Mss","40",
                      "-ns","0.1",
+                     "-n","debugging"
                     ],
             "console": "integratedTerminal",
             "env": {
diff --git a/main.py b/main.py
index 6cb71fb..bf3e27b 100644
--- a/main.py
+++ b/main.py
@@ -25,17 +25,19 @@ def parse_args():
 	parser.add_argument('-e','--n_epoch',type = int, default = 100,help = 'number of total epochs')
 	# parser.add_argument('-ms','--ms',type = float, default = None, help = 'ratio of labeled source imagesper batch')
 	parser.add_argument('-mss','--ms_list',type = float, nargs = "+", default = None, help = 'list of ratio of labeled source images')
-	# parser.add_argument('-mt','--mt',type = float, default = None, help = 'ratio of labeled target images')
+	parser.add_argument('-Mss','--Ms_list',type = int, nargs = "+", default = None, help = 'list of total number of labeled source images')
+	parser.add_argument('-Mt','--Mt',type = int, default = 25, help = 'ratio of labeled target images')
 	# parser.add_argument('-mts','--mt_list',type = float, nargs = "+", default = None, help = 'list of ratio of labeled target images')
 	# parser.add_argument('-dc_dim','--dc_dim',type = int, default = 100, help = 'dimension of the domain classifier network')
 	# parser.add_argument('-log','--log_wandb',type=bool, default= True, help="whether to log to wandb or not")
 	# parser.add_argument('-l','--layers',type=int, help="number of layers")
 	parser.add_argument('-ll','--layers_list',type=int, nargs = "+", help="list of number of layers")
 	# parser.add_argument('-rn','--run_name',type=str, help="name of the run")
-	parser.add_argument('-Mts','--Mt_list',type = int, nargs = "+", default = None, help = 'list of number of labeled target images per batch')
+	# parser.add_argument('-Mts','--Mt_list',type = int, nargs = "+", default = None, help = 'list of number of labeled target images per batch')
 	parser.add_argument('-dcms','--dimchange_multipliers',type = float, nargs = "+", default = None, help = 'list of dimchange multipliers. common for conv and linear layers.')
-	parser.add_argument('-beta','--beta',type = float,default = None, help = 'balance parameter between classfication and domain losses. beta =1 means zero contribution from domain loss.')
+	parser.add_argument('-beta','--beta',type = float,default = 0.5, help = 'balance parameter between classfication and domain losses. beta =1 means zero contribution from domain loss.')
 	parser.add_argument('-ns','--ns',type = float, default = None, help ='the ratio that determines the dataset length used')
+	parser.add_argument('-r','--repeats',type = int, default = 1, help ='how many repeats')
 	return parser.parse_args()
 
 
@@ -54,186 +56,192 @@ if __name__ == '__main__':
 	n_epoch = args.n_epoch
 	beta = args.beta 
 	ns = args.ns
-	Mt = 1
+	Mt = args.Mt 
 
 
-
-	for layer in args.layers_list:
-		for dcm in args.dimchange_multipliers:
-			for ms in args.ms_list:
-				Ms = int(ms*batch_size)
-				manual_seed = random.randint(1, 10000)
-				random.seed(manual_seed)
-				torch.manual_seed(manual_seed)
+	for _ in range(args.repeats):
+		for layer in args.layers_list:
+			for dcm in args.dimchange_multipliers:
+				for Ms in args.Ms_list:
+					# Ms = int(ms*batch_size)
+					manual_seed = random.randint(1, 10000)
+					random.seed(manual_seed)
+					torch.manual_seed(manual_seed)
             
 
 
-				# logging - (wandb)
+					# logging - (wandb)
+	
+					# run_name = get_run_name() if not args.run_name else args.run_name
+					# run_folder = '/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/runs/' + run_name
+					# os.makedirs(run_folder, exist_ok=True)
 
-				# run_name = get_run_name() if not args.run_name else args.run_name
-				# run_folder = '/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/runs/' + run_name
-				# os.makedirs(run_folder, exist_ok=True)
 
+					wandb_kwargs = {# "dir": run_folder,
+							# "name": run_name,
+							"project": args.project_name,
+							"notes": args.notes,
+							# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
+							#"resume": 'allow',
+							#"allow_val_change": True,
+							"config":{"Ms": Ms, "Mt": Mt, "layer":layer, "beta":beta, "dcm":dcm, "ns":ns, "bs":batch_size}
+							}
+					
 
-				wandb_kwargs = {# "dir": run_folder,
-						# "name": run_name,
-						"project": args.project_name,
-						"notes": args.notes,
-						# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
-						#"resume": 'allow',
-						#"allow_val_change": True,
-						"config":{"ms": ms, "Mt": Mt, "layer":layer, "beta":beta, "dcm":dcm, "ns":ns, "bs":batch_size}
-						}
-				
+					# Logging setup (You can replace it with your preferred logging method)
+					wandb.init(**wandb_kwargs)
+					wandb.run.log_code('.')
 
-				# Logging setup (You can replace it with your preferred logging method)
-				wandb.init(**wandb_kwargs)
-				wandb.run.log_code('.')
 
+					# load data
 
-				# load data
+					img_transform_source = transforms.Compose([
+						transforms.Resize(image_size),
+						transforms.ToTensor(),
+						transforms.Normalize(mean=(0.1307,), std=(0.3081,))
+					])
 
-				img_transform_source = transforms.Compose([
-					transforms.Resize(image_size),
-					transforms.ToTensor(),
-					transforms.Normalize(mean=(0.1307,), std=(0.3081,))
-				])
+					img_transform_target = transforms.Compose([
+						transforms.Resize(image_size),
+						transforms.ToTensor(),
+						transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
+					])
 
-				img_transform_target = transforms.Compose([
-					transforms.Resize(image_size),
-					transforms.ToTensor(),
-					transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
-				])
+					dataset_source = datasets.MNIST(
+						root='dataset',
+						train=True,
+						transform=img_transform_source,
+						download=True
+					)
 
-				dataset_source = datasets.MNIST(
-					root='dataset',
-					train=True,
-					transform=img_transform_source,
-					download=True
-				)
+					dataloader_source = torch.utils.data.DataLoader(
+						dataset=dataset_source,
+						batch_size=batch_size,
+						shuffle=True,
+						num_workers=8,
+						drop_last = True)
 
-				dataloader_source = torch.utils.data.DataLoader(
-					dataset=dataset_source,
-					batch_size=batch_size,
-					shuffle=True,
-					num_workers=8,
-					drop_last = True)
+					train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')
 
-				train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')
+					dataset_target = GetLoader(
+						data_root=os.path.join(target_image_root, 'mnist_m_train'),
+						data_list=train_list,
+						transform=img_transform_target
+					)
 
-				dataset_target = GetLoader(
-					data_root=os.path.join(target_image_root, 'mnist_m_train'),
-					data_list=train_list,
-					transform=img_transform_target
-				)
+					dataloader_target = torch.utils.data.DataLoader(
+						dataset=dataset_target,
+						batch_size=batch_size,
+						shuffle=True,
+						num_workers=8,
+						drop_last = True)
 
-				dataloader_target = torch.utils.data.DataLoader(
-					dataset=dataset_target,
-					batch_size=batch_size,
-					shuffle=True,
-					num_workers=8,
-					drop_last = True)
+					# load model
+	
+					my_net = CNNModel(layer,dcm)
 
-				# load model
+					# setup optimizer
 
-				my_net = CNNModel(layer,dcm)
+					optimizer = optim.Adam(my_net.parameters(), lr=lr)
 
-				# setup optimizer
+					loss_class = torch.nn.NLLLoss(reduction = "sum")
+					loss_domain = torch.nn.NLLLoss(reduction = "sum")
 
-				optimizer = optim.Adam(my_net.parameters(), lr=lr)
+					if cuda:
+						my_net = my_net.cuda()
+						loss_class = loss_class.cuda()
+						loss_domain = loss_domain.cuda()
 
-				loss_class = torch.nn.NLLLoss(reduction = "sum")
-				loss_domain = torch.nn.NLLLoss(reduction = "sum")
+					for p in my_net.parameters():
+						p.requires_grad = True
+					
 
-				if cuda:
-					my_net = my_net.cuda()
-					loss_class = loss_class.cuda()
-					loss_domain = loss_domain.cuda()
+					# training
+					best_accu_t = 0.0
+					for epoch in range(n_epoch):
 
-				for p in my_net.parameters():
-					p.requires_grad = True
-					
+						len_dataloader = int(ns*(min(len(dataloader_source), len(dataloader_target))))
+						data_source_iter = iter(dataloader_source)
+						data_target_iter = iter(dataloader_target)
 
-				# training
-				best_accu_t = 0.0
-				for epoch in range(n_epoch):
+						Msx_list = distribute_apples(Ms-1,len_dataloader-1)
+						Msx_list.append(1)
+						Mtx_list = distribute_apples(Mt-1,len_dataloader-1)
+						Mtx_list.append(1)
 
-					len_dataloader = int(ns*(min(len(dataloader_source), len(dataloader_target))))
-					data_source_iter = iter(dataloader_source)
-					data_target_iter = iter(dataloader_target)
+						for i in range(len_dataloader):
 
-					for i in range(len_dataloader):
+							Msx = Msx_list[i] # required number of labeled source data for per batch to ensure total of Ms 
+							Mtx = Mtx_list[i]
 
-						p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader
-						alpha = 2. / (1. + np.exp(-10 * p)) - 1
+							p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader
+							alpha = 2. / (1. + np.exp(-10 * p)) - 1
 
-						# training model using source data
-						data_source = data_source_iter.next()
-						s_img, s_label = data_source
+							# training model using source data
+							data_source = data_source_iter.next()
+							s_img, s_label = data_source
 
-						my_net.zero_grad()
-						batch_size = len(s_label)
+							my_net.zero_grad()
+							batch_size = len(s_label)
 
-						domain_label = torch.zeros(batch_size).long()
+							domain_label = torch.zeros(batch_size).long()
 
-						if cuda:
-							s_img = s_img.cuda()
-							s_label = s_label.cuda()
-							domain_label = domain_label.cuda()
+							if cuda:
+								s_img = s_img.cuda()
+								s_label = s_label.cuda()
+								domain_label = domain_label.cuda()
 
 
-						class_output, domain_output = my_net(input_data=s_img, alpha=alpha)
-						err_s_label = loss_class(class_output[:Ms,:], s_label[:Ms])
-						err_s_domain = loss_domain(domain_output, domain_label)
+							class_output, domain_output = my_net(input_data=s_img, alpha=alpha)
+							err_s_label = loss_class(class_output[:Msx,:], s_label[:Msx])
+							err_s_domain = loss_domain(domain_output, domain_label)
 
-						# training model using target data
-						data_target = data_target_iter.next()
-						t_img, t_label = data_target
+							# training model using target data
+							data_target = data_target_iter.next()
+							t_img, t_label = data_target
 
-						batch_size = len(t_img)
+							batch_size = len(t_img)
 
-						domain_label = torch.ones(batch_size).long()
+							domain_label = torch.ones(batch_size).long()
 
-						if cuda:
-							t_img = t_img.cuda()
-							t_label = t_label.cuda()
-							domain_label = domain_label.cuda()
+							if cuda:
+								t_img = t_img.cuda()
+								t_label = t_label.cuda()
+								domain_label = domain_label.cuda()
 
-						class_output, domain_output = my_net(input_data=t_img, alpha=alpha)
-						err_t_label = loss_class(class_output[:Mt,:], t_label[:Mt])
-						err_t_domain = loss_domain(domain_output, domain_label)
-						err =(1-beta)*(err_t_domain + err_s_domain) + beta*(err_s_label + err_t_label)
-						err.backward()
-						optimizer.step()
+							class_output, domain_output = my_net(input_data=t_img, alpha=alpha)
+							err_t_label = loss_class(class_output[:Mtx,:], t_label[:Mtx])
+							err_t_domain = loss_domain(domain_output, domain_label)
+							err =(1-beta)*(err_t_domain + err_s_domain) + beta*(err_s_label + err_t_label)
+							err.backward()
+							optimizer.step()
 
-						# sys.stdout.write('\r epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \
-						#       % (epoch, i + 1, len_dataloader, err_s_label.data.cpu().numpy(),
-						#          err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()))
-						# sys.stdout.flush()
-						torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))
+							# sys.stdout.write('\r epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \
+							#       % (epoch, i + 1, len_dataloader, err_s_label.data.cpu().numpy(),
+							#          err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()))
+							# sys.stdout.flush()
+							torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))
 
-					print('\n')
-					print('ms: %.2f | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (ms,Mt,dcm,layer,epoch,n_epoch))
-					accu_s = test(source_dataset_name)
-					print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))
-					accu_t = test(target_dataset_name)
-					print('Accuracy of the %s dataset: %f\n' % ('mnist_m', accu_t))
+						print('\n')
+						print('Ms: %d | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (Ms,Mt,dcm,layer,epoch,n_epoch))
+						accu_s = test(source_dataset_name)
+						print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))
+						accu_t = test(target_dataset_name)
+						print('Accuracy of the %s dataset: %f\n' % ('mnist_m', accu_t))
 
-					if accu_t > best_accu_t:
-						best_accu_s = accu_s
-						best_accu_t = accu_t
-						torch.save(my_net, '{0}/mnist_mnistm_model_epoch_best.pth'.format(model_root))
+						if accu_t > best_accu_t:
+							best_accu_s = accu_s
+							best_accu_t = accu_t
+							torch.save(my_net, '{0}/mnist_mnistm_model_epoch_best.pth'.format(model_root))
 
-					wandb.log({"err_s_label_train": err_s_label,
-					"err_s_domain_train": err_s_domain,
-					"err_t_label_train": err_t_label,
-					"err_t_domain_train": err_t_domain,
-					"accu_s_test": accu_s,
-					"accu_t_test": accu_t}, step=epoch)
-					
-				wandb.finish()
+						wandb.log({"err_s_label_train": err_s_label/Msx,
+						"err_s_domain_train": err_s_domain/batch_size,
+						"err_t_label_train": err_t_label/Mtx,
+						"err_t_domain_train": err_t_domain/batch_size,
+						"total_loss":err,
+						"accu_s_test": accu_s,
+						"accu_t_test": accu_t}, step=epoch)
 
-				# print('============ Summary ============= \n')
-				# print('Accuracy of the %s dataset: %f' % ('mnist', best_accu_s))
-				# print('Accuracy of the %s dataset: %f' % ('mnist_m', best_accu_t))
-				# print('Corresponding model was save in ' + model_root + '/mnist_mnistm_model_epoch_best.pth')
+					
+					wandb.finish()
+					
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 9f12437..343a14f 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240309_163923-tb3agb8f/logs/debug-internal.log
\ No newline at end of file
+run-20240312_173202-2vdo1u1t/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 33d611c..f1315bf 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240309_163923-tb3agb8f/logs/debug.log
\ No newline at end of file
+run-20240312_173202-2vdo1u1t/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index eb6c6cb..f7ecf83 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240309_163923-tb3agb8f
\ No newline at end of file
+run-20240312_173202-2vdo1u1t
\ No newline at end of file
