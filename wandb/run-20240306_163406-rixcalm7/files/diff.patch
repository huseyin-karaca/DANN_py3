diff --git a/.vscode/launch.json b/.vscode/launch.json
index 7a1a92a..289c379 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -18,8 +18,11 @@
             "request": "launch",
             "cwd": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3",
             "program": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/main.py",
-            "args": [//"-bs", "512", 
-                     //"-e","10",
+            "args": [// "-bs", "512", 
+                     "-ll","2",
+                     "-dcms","5", "4",
+                     "-mss","0.1",
+                     "-ns","0.1",
                     ],
             "console": "integratedTerminal",
             "env": {
diff --git a/huseyin_functions.py b/huseyin_functions.py
index bfef860..82b89d2 100644
--- a/huseyin_functions.py
+++ b/huseyin_functions.py
@@ -11,3 +11,34 @@ def get_run_name():
             return character_name
         else:
             return None
+
+from torch.nn.modules.module import _addindent
+import torch
+import numpy as np
+
+def torch_summarize(model, show_weights=True, show_parameters=True):
+    """Summarizes torch model by showing trainable parameters and weights."""
+    tmpstr = model.__class__.__name__ + ' (\n'
+    for key, module in model._modules.items():
+        # if it contains layers let call it recursively to get params and weights
+        if type(module) in [
+            torch.nn.modules.container.Container,
+            torch.nn.modules.container.Sequential
+        ]:
+            modstr = torch_summarize(module)
+        else:
+            modstr = module.__repr__()
+        modstr = _addindent(modstr, 2)
+
+        params = sum([np.prod(p.size()) for p in module.parameters()])
+        weights = tuple([tuple(p.size()) for p in module.parameters()])
+
+        tmpstr += '  (' + key + '): ' + modstr 
+        if show_weights:
+            tmpstr += ', weights={}'.format(weights)
+        if show_parameters:
+            tmpstr +=  ', parameters={}'.format(params)
+        tmpstr += '\n'   
+
+    tmpstr = tmpstr + ')'
+    return tmpstr
\ No newline at end of file
diff --git a/main.py b/main.py
index 09d858c..1169a84 100644
--- a/main.py
+++ b/main.py
@@ -14,11 +14,12 @@ from huseyin_functions import get_run_name
 import argparse
 import wandb
 
+
 # main code
 
 def parse_args():
 	parser = argparse.ArgumentParser()
-	# parser.add_argument('-bs', '--batch_size', type=int, default=128, help='batch size to train on (default: 8)')
+	parser.add_argument('-bs', '--batch_size', type=int, default=128, help='batch size to train on (default: 8)')
 	parser.add_argument('-n','--notes',type=str, default = None , help = 'wandb run notes')
 	parser.add_argument('-pn','--project_name',type=str, default = "nsubat" , help = 'wandb project name')
 	parser.add_argument('-e','--n_epoch',type = int, default = 100,help = 'number of total epochs')
@@ -32,7 +33,9 @@ def parse_args():
 	parser.add_argument('-ll','--layers_list',type=int, nargs = "+", help="list of number of layers")
 	# parser.add_argument('-rn','--run_name',type=str, help="name of the run")
 	parser.add_argument('-Mts','--Mt_list',type = int, nargs = "+", default = None, help = 'list of number of labeled target images per batch')
+	parser.add_argument('-dcms','--dimchange_multipliers',type = float, nargs = "+", default = None, help = 'list of dimchange multipliers. common for conv and linear layers.')
 	parser.add_argument('-beta','--beta',type = float,default = None, help = 'balance parameter between classfication and domain losses. beta =1 means zero contribution from domain loss.')
+	parser.add_argument('-ns','--ns',type = float, default = None, help ='the ratio that determines the dataset length used')
 	return parser.parse_args()
 
 
@@ -46,21 +49,23 @@ if __name__ == '__main__':
 	cuda = True
 	cudnn.benchmark = True
 	lr = 1e-3
-	batch_size = 256
+	batch_size = args.batch_size
 	image_size = 28
 	n_epoch = args.n_epoch
 	beta = args.beta 
+	ns = args.ns
+	Mt = 1
 
 
 
 	for layer in args.layers_list:
-		for Mt in args.Mt_list:
+		for dcm in args.dimchange_multipliers:
 			for ms in args.ms_list:
 				Ms = int(ms*batch_size)
 				manual_seed = random.randint(1, 10000)
 				random.seed(manual_seed)
 				torch.manual_seed(manual_seed)
-
+            
 
 
 				# logging - (wandb)
@@ -77,8 +82,9 @@ if __name__ == '__main__':
 						# "id": run_name, #wandb_id_finder_from_folder(self.run_folder) if args.mode == 'resume' else wandb.util.generate_id(),
 						#"resume": 'allow',
 						#"allow_val_change": True,
-						"config":{"ms": ms, "Mt": Mt, "layer":layer, "beta":beta}
+						"config":{"ms": ms, "Mt": Mt, "layer":layer, "beta":beta, "dcm":dcm, "ns":ns, "bs":batch_size}
 						}
+				
 
 				# Logging setup (You can replace it with your preferred logging method)
 				wandb.init(**wandb_kwargs)
@@ -130,7 +136,7 @@ if __name__ == '__main__':
 
 				# load model
 
-				my_net = CNNModel(layer)
+				my_net = CNNModel(layer,dcm)
 
 				# setup optimizer
 
@@ -151,7 +157,7 @@ if __name__ == '__main__':
 				best_accu_t = 0.0
 				for epoch in range(n_epoch):
 
-					len_dataloader = min(len(dataloader_source), len(dataloader_target))
+					len_dataloader = int(ns*(min(len(dataloader_source), len(dataloader_target))))
 					data_source_iter = iter(dataloader_source)
 					data_target_iter = iter(dataloader_target)
 
@@ -206,7 +212,7 @@ if __name__ == '__main__':
 						torch.save(my_net, '{0}/mnist_mnistm_model_epoch_current.pth'.format(model_root))
 
 					print('\n')
-					print('ms: %.2f | Mt: %d | Epoch: %d' % (ms,Mt,epoch))
+					print('ms: %.2f | Mt: %d | dcm: %.2f | layer: %d | Epoch: %d/%d' % (ms,Mt,dcm,layer,epoch,n_epoch))
 					accu_s = test(source_dataset_name)
 					print('Accuracy of the %s dataset: %f' % ('mnist', accu_s))
 					accu_t = test(target_dataset_name)
diff --git a/model.py b/model.py
index 4c65e91..0742dfd 100644
--- a/model.py
+++ b/model.py
@@ -1,23 +1,27 @@
 import torch.nn as nn
 from functions import ReverseLayerF
+from torchsummary import summary
 
 
 class CNNModel(nn.Module):
 
-    def __init__(self,layers):
+    def __init__(self,layers, dimchange_multiplier):
         super(CNNModel, self).__init__()
+        dim_conv = int(64 * dimchange_multiplier)
+        dim_lin = int(100 * dimchange_multiplier)
+
         self.feature = nn.Sequential()
-        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))
-        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))
+        self.feature.add_module('f_conv1', nn.Conv2d(3, dim_conv, kernel_size=5))
+        self.feature.add_module('f_bn1', nn.BatchNorm2d(dim_conv))
         self.feature.add_module('f_pool1', nn.MaxPool2d(2))
         self.feature.add_module('f_relu1', nn.ReLU(True))
 
         for ii in range(layers-2):
-            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(64,64,padding = 2,kernel_size=5))
-            self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(64))
+            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(dim_conv,dim_conv,padding = 2,kernel_size=5))
+            self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(dim_conv))
             self.feature.add_module(f'f_relu{2+ii}', nn.ReLU(True))
  
-        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(64, 50, kernel_size=5))
+        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(dim_conv, 50, kernel_size=5))
         self.feature.add_module(f'f_bn{layers}', nn.BatchNorm2d(50))
         self.feature.add_module(f'f_drop{layers}', nn.Dropout2d())
         self.feature.add_module(f'f_pool{layers}', nn.MaxPool2d(2))
@@ -35,14 +39,14 @@ class CNNModel(nn.Module):
         self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))
 
         self.domain_classifier = nn.Sequential()
-        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, 100))
-        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))
+        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, dim_lin))
+        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(dim_lin))
         self.domain_classifier.add_module('d_relu1', nn.ReLU(True))
         for ii in range(layers-2): 
-            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(100, 100)) 
-            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(100))
+            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(dim_lin, dim_lin)) 
+            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(dim_lin))
             self.domain_classifier.add_module(f'd_relu{2+ii}', nn.ReLU(True))
-        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(100, 2))
+        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(dim_lin, 2))
         self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))
 
     def forward(self, input_data, alpha):
@@ -54,54 +58,3 @@ class CNNModel(nn.Module):
         domain_output = self.domain_classifier(reverse_feature)
 
         return class_output, domain_output
-
-class CNNModel_old(nn.Module):
-
-    def __init__(self,layers):
-        super(CNNModel, self).__init__()
-        self.feature = nn.Sequential()
-        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))
-        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))
-        self.feature.add_module('f_pool1', nn.MaxPool2d(2))
-        self.feature.add_module('f_relu1', nn.ReLU(True))
-        for ii in range(layers-2):
-            self.feature.add_module(f'f_conv{2+ii}', nn.Conv2d(64,64,padding = 2,kernel_size=5))
-            self.feature.add_module(f'f_bn{2+ii}', nn.BatchNorm2d(64))
-            self.feature.add_module(f'f_relu{2+ii}', nn.ReLU(True))
-        self.feature.add_module(f'f_conv{layers}', nn.Conv2d(64, 50, kernel_size=5))
-        self.feature.add_module(f'f_bn{layers}', nn.BatchNorm2d(50))
-        self.feature.add_module(f'f_drop{layers}', nn.Dropout2d())
-        self.feature.add_module(f'f_pool{layers}', nn.MaxPool2d(2))
-        self.feature.add_module(f'f_relu{layers}', nn.ReLU(True))
-
-        self.class_classifier = nn.Sequential()
-        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 5 * 5, 100))
-        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))
-        self.class_classifier.add_module('c_relu1', nn.ReLU(True))
-        self.class_classifier.add_module('c_drop1', nn.Dropout())
-        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))
-        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))
-        self.class_classifier.add_module('c_relu2', nn.ReLU(True))
-        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))
-        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))
-
-        self.domain_classifier = nn.Sequential()
-        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 5 * 5, 100)) # bak buralar değişiyor 50*4*4 ten 50*5*5 oluyorlar.
-        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))
-        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))
-        for ii in range(layers-2): 
-            self.domain_classifier.add_module(f'd_fc{2+ii}', nn.Linear(100, 100)) 
-            self.domain_classifier.add_module(f'd_bn{2+ii}', nn.BatchNorm1d(100))
-            self.domain_classifier.add_module(f'd_relu{2+ii}', nn.ReLU(True))
-        self.domain_classifier.add_module(f'd_fc{layers}', nn.Linear(100, 2))
-        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))
-
-    def forward(self, input_data, gamma, ms_or_mt):
-        bs = input_data.data.shape[0]
-        Ms_or_Mt = int(bs*ms_or_mt)
-
-        input_data = input_data.expand(bs, 3, 32, 32)
-        feature = self.feature(input_data)
-        feature = feature.view(-1, 50 * 5 * 5) # bak burada da var bir değişikliki
-        reverse_feature = ReverseLayerF.apply(feature, gamma)
-        class_output = self.class_classifier(feature[0:Ms_or_Mt,:])
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 51f95ac..ec1af2f 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240209_165914-h2i2ay0n/logs/debug-internal.log
\ No newline at end of file
+run-20240306_163406-rixcalm7/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 73290ea..1671560 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240209_165914-h2i2ay0n/logs/debug.log
\ No newline at end of file
+run-20240306_163406-rixcalm7/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 734054b..48eabe0 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240209_165914-h2i2ay0n
\ No newline at end of file
+run-20240306_163406-rixcalm7
\ No newline at end of file
