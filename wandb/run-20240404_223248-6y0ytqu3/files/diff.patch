diff --git a/.vscode/launch.json b/.vscode/launch.json
index 135a182..79430b6 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -19,14 +19,15 @@
             "cwd": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3",
             "program": "/home/huseyin/fungtion/dannpy_yeniden/DANN_py3/main.py",
             "args": ["-bs", "128", 
-                     "-ll","5",
-                     "-dcms","3",
-                     "-Mss","20","30","40",
-                     "-Mt","10",
-                     "-ns","0.1",
-                     "-r","2",
-                     "-e","300",
-                     "-n","debugging"
+                     "-ll","2",
+                     "-dcms","0.25","1","2","4","8",
+                     "-Mss","120","60","80","100",
+                     "-Mt","20",
+                     "-Ns","6000",
+                     "-Nt","6000",
+                     "-r","1",
+                     "-e","5",
+                     "-pn","debugging"
                     ],
             "console": "integratedTerminal",
             "env": {
diff --git a/deneme.py b/deneme.py
index e69de29..362d6d3 100644
--- a/deneme.py
+++ b/deneme.py
@@ -0,0 +1,120 @@
+import torch
+from torch.utils.data import RandomSampler, BatchSampler, DataLoader, random_split, Subset
+import numpy as np
+from torchvision import datasets, transforms
+from huseyin_functions import get_run_name, distribute_apples
+import random, os
+from data_loader import GetLoader
+
+Ns = 50000
+Nt = 50000
+batch_size = 128
+image_size = 28
+
+Ms = 100
+Mt = 25
+
+n_epoch = 300
+best_accu_t = 0.0
+
+source_dataset_name = 'MNIST'
+target_dataset_name = 'mnist_m'
+source_image_root = os.path.join('dataset', source_dataset_name)
+target_image_root = os.path.join('dataset', target_dataset_name)
+
+img_transform_source = transforms.Compose([
+    transforms.Resize(image_size),
+    transforms.ToTensor(),
+    transforms.Normalize(mean=(0.1307,), std=(0.3081,))
+])
+
+
+img_transform_target = transforms.Compose([
+    transforms.Resize(image_size),
+    transforms.ToTensor(),
+    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
+])
+
+
+
+Ks = int(np.floor(Ns/batch_size)) # former len_dataloader. how many batches are there in every epoch?
+Kt = int(np.floor(Nt/batch_size))
+
+Msx_list = distribute_apples(Ms,Ks)
+Mtx_list = distribute_apples(Mt,Kt)
+
+
+# read source and target datasets for the first time:
+dataset_source = datasets.MNIST(
+    root='dataset',
+    train=True,
+    transform=img_transform_source,
+    download=True
+)
+
+dataset_target = GetLoader(
+    data_root=os.path.join(target_image_root, 'mnist_m_train'),
+    data_list=os.path.join(target_image_root, 'mnist_m_train_labels.txt'),
+    transform=img_transform_target
+)
+
+# create subsets of the original source and target datasets. choose random Ns and Nt indices respectively:
+dataset_source = Subset(dataset_source, indices=random.sample(range(len(dataset_source)),Ns))
+dataset_target = Subset(dataset_target, indices=random.sample(range(len(dataset_target)),Nt))
+
+# split labeled and unlabeled datasets:
+dataset_source_labeled, dataset_source_unlabeled = random_split(dataset_source, [Ms, Ns-Ms])
+dataset_target_labeled, dataset_target_unlabeled = random_split(dataset_target, [Mt, Nt-Mt])
+
+# create labeled and unlabeled dataloaders seperately:
+# (in order to prevent using more labeled total data than specified Ms and Mt values)
+
+# labeled source dataloader's batch size is max(Msx_list) as Msx_list defines the 
+# distribution of number of labeled samples per batch. when creating merged source batch, 
+# we will replace the last max(Msx_list) image of the unlabeled source batch with the 
+# images coming from the labeled dataloader. 
+# target dataloaders will also undergo the same process.
+
+# source dataloaders
+dataloader_source_labeled = DataLoader(
+    dataset=dataset_source_labeled,
+    batch_size=max(Msx_list),
+    shuffle=True,
+    num_workers=8,
+    drop_last = True)
+    
+dataloader_source_unlabeled = DataLoader(
+    dataset=dataset_source_unlabeled,
+    batch_size=batch_size,
+    shuffle=True,
+    num_workers=8,
+    drop_last = True)
+
+# target dataloaders
+dataloader_target_labeled = DataLoader(
+    dataset=dataset_target_labeled,
+    batch_size=max(Mtx_list),
+    shuffle=True,
+    num_workers=8,
+    drop_last = True)
+    
+dataloader_target_unlabeled = DataLoader(
+    dataset=dataset_target_unlabeled,
+    batch_size=batch_size,
+    shuffle=True,
+    num_workers=8,
+    drop_last = True)
+
+# training
+
+for epoch in range(n_epoch):
+
+    source_labeled_iter = iter(dataloader_source_labeled)
+    source_unlabeled_iter = iter(dataloader_source_unlabeled)
+
+    for i in range(min(Ks,Kt)): # Ks, Kt: dataloader lengths if a non-iterative dataloader would have been used
+        Msx = Msx_list[i] # required number of labeled source data for per batch to ensure total of Ms 
+        Mtx = Mtx_list[i] 
+
+        source_labeled_data = source_labeled_iter.next()
+        source_unlabeled_data = source_unlabeled_iter.next()
diff --git a/huseyin_functions.py b/huseyin_functions.py
index e74edab..64cd0a2 100644
--- a/huseyin_functions.py
+++ b/huseyin_functions.py
@@ -1,3 +1,4 @@
+import random 
 def get_run_name():
     with open("/home/huseyin/fungtion/DANN_py3/ilveilceler.txt", "r+", encoding="utf-8") as file:
         lines = file.readlines()
@@ -73,5 +74,40 @@ def distribute_apples(M, N):
       if distribution[i] < ideal_apples_per_slot:
         distribution[i] += 1
         break
-    
-  return distribution
\ No newline at end of file
+  
+  random.shuffle(distribution)
+  return distribution
+
+def distribute_apples_new(M, K):
+  """
+  M tane elmayı K tane sepete olabildiğince eşit sayıda dağıtır.
+
+  Parametreler:
+    M (int): Elma sayısı.
+    K (int): Sepet sayısı.
+
+  Dönüş değeri:
+    List[int]: Her sepete düşen elma sayılarının listesi.
+  """
+
+  # Her sepete düşen ortalama elma sayısını hesapla
+  average_apples = M // K
+
+  # Her sepete ortalama sayıda elma koy
+  apple_distribution = [average_apples] * K
+
+  # Kalan elmaları rastgele sepetlere dağıt
+  remaining_apples = M - K * average_apples
+  for _ in range(remaining_apples):
+    # Elma alacak rastgele bir sepet seç
+    random_basket = random.randint(0, K - 1)
+
+    # Seçilen sepete bir elma ekle
+    apple_distribution[random_basket] += 1
+
+  # Son sepetin en az bir elma almasını sağla
+  if apple_distribution[-1] == 0:
+    apple_distribution[-1] = 1
+    apple_distribution[random.randint(0, K - 2)] -= 1
+
+  return apple_distribution
\ No newline at end of file
diff --git a/main.py b/main.py
index 55cf872..81c56f6 100644
--- a/main.py
+++ b/main.py
@@ -3,7 +3,7 @@ import os
 import sys
 import torch.backends.cudnn as cudnn
 import torch.optim as optim
-import torch.utils.data
+from torch.utils.data import RandomSampler, BatchSampler, DataLoader, random_split, Subset
 import numpy as np
 from data_loader import GetLoader
 from torchvision import datasets
@@ -13,6 +13,8 @@ from test import test
 from huseyin_functions import get_run_name, distribute_apples
 import argparse
 import wandb
+import torch
+import matplotlib.pyplot as plt
 
 
 # main code
@@ -36,7 +38,8 @@ def parse_args():
 	# parser.add_argument('-Mts','--Mt_list',type = int, nargs = "+", default = None, help = 'list of number of labeled target images per batch')
 	parser.add_argument('-dcms','--dimchange_multipliers',type = float, nargs = "+", default = None, help = 'list of dimchange multipliers. common for conv and linear layers.')
 	parser.add_argument('-beta','--beta',type = float,default = 0.5, help = 'balance parameter between classfication and domain losses. beta =1 means zero contribution from domain loss.')
-	parser.add_argument('-ns','--ns',type = float, default = None, help ='the ratio that determines the dataset length used')
+	parser.add_argument('-Ns','--Ns',type = int, default = None, help ='the total number of data used in source')
+	parser.add_argument('-Nt','--Nt',type = int, default = None, help ='the total number of data used in target')
 	parser.add_argument('-r','--repeats',type = int, default = 1, help ='how many repeats')
 	return parser.parse_args()
 
@@ -55,8 +58,9 @@ if __name__ == '__main__':
 	image_size = 28
 	n_epoch = args.n_epoch
 	beta = args.beta 
-	ns = args.ns
-	Mt = args.Mt 
+	Ns = args.Ns
+	Mt = args.Mt
+	Nt = args.Nt 
 
 
 	for repeat in range(args.repeats):
@@ -89,7 +93,8 @@ if __name__ == '__main__':
 									  "layer":layer, 
 									  "beta":beta, 
 									  "dcm":dcm, 
-									  "ns":ns, 
+									  "Ns":Ns, 
+									  "Nt":Nt,
 									  "bs":batch_size,
 									  "repeat":repeat+1}
 							}
@@ -102,6 +107,7 @@ if __name__ == '__main__':
 
 					# load data
 
+					## define transformations
 					img_transform_source = transforms.Compose([
 						transforms.Resize(image_size),
 						transforms.ToTensor(),
@@ -114,6 +120,7 @@ if __name__ == '__main__':
 						transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
 					])
 
+					## read source and target datasets for the first time:
 					dataset_source = datasets.MNIST(
 						root='dataset',
 						train=True,
@@ -121,23 +128,57 @@ if __name__ == '__main__':
 						download=True
 					)
 
-					dataloader_source = torch.utils.data.DataLoader(
-						dataset=dataset_source,
-						batch_size=batch_size,
-						shuffle=True,
-						num_workers=8,
-						drop_last = True)
-
-					train_list = os.path.join(target_image_root, 'mnist_m_train_labels.txt')
-
 					dataset_target = GetLoader(
 						data_root=os.path.join(target_image_root, 'mnist_m_train'),
-						data_list=train_list,
+						data_list=os.path.join(target_image_root, 'mnist_m_train_labels.txt'),
 						transform=img_transform_target
 					)
 
-					dataloader_target = torch.utils.data.DataLoader(
-						dataset=dataset_target,
+					## create subsets of the original source and target datasets. choose random Ns and Nt indices respectively:
+					dataset_source = Subset(dataset_source, indices=random.sample(range(len(dataset_source)),Ns))
+					dataset_target = Subset(dataset_target, indices=random.sample(range(len(dataset_target)),Nt))
+
+					## split labeled and unlabeled datasets:
+					dataset_source_labeled, dataset_source_unlabeled = random_split(dataset_source, [Ms, Ns-Ms])
+					dataset_target_labeled, dataset_target_unlabeled = random_split(dataset_target, [Mt, Nt-Mt])
+
+
+					## create labeled and unlabeled dataloaders seperately:
+					## (in order to prevent using more labeled total data than specified Ms and Mt values)
+
+					### how many batches are there in each epoch? former len_dataloader. min(Ks,Kt) will be used
+					### for total number of iterations of each epoch. (this is necessary to maintain epoch structure)
+					Ks = int(np.floor(Ns/batch_size)) 
+					Kt = int(np.floor(Nt/batch_size))
+					K = min(Ks,Kt) 
+
+
+
+					### source dataloaders
+					dataloader_source_labeled = DataLoader(
+						dataset=dataset_source_labeled,
+						batch_size=1,
+						shuffle=True,
+						num_workers=8,
+						drop_last = True)
+						
+					dataloader_source_unlabeled = DataLoader(
+						dataset=dataset_source_unlabeled,
+						batch_size=batch_size,
+						shuffle=True,
+						num_workers=8,
+						drop_last = True)
+
+					### target dataloaders
+					dataloader_target_labeled = DataLoader(
+						dataset=dataset_target_labeled,
+						batch_size=1,
+						shuffle=True,
+						num_workers=8,
+						drop_last = True)
+						
+					dataloader_target_unlabeled = DataLoader(
+						dataset=dataset_target_unlabeled,
 						batch_size=batch_size,
 						shuffle=True,
 						num_workers=8,
@@ -167,31 +208,51 @@ if __name__ == '__main__':
 					best_accu_t = 0.0
 					for epoch in range(n_epoch):
 
-						len_dataloader = int(ns*(min(len(dataloader_source), len(dataloader_target))))
-						data_source_iter = iter(dataloader_source)
-						data_target_iter = iter(dataloader_target)
-
-						Msx_list = distribute_apples(Ms-1,len_dataloader-1)
+						### distribution of number of labeled samples per batch
+						Msx_list = distribute_apples(Ms-1,Ks-1)
+						Mtx_list = distribute_apples(Mt-1,Kt-1)
 						Msx_list.append(1)
-						Mtx_list = distribute_apples(Mt-1,len_dataloader-1)
 						Mtx_list.append(1)
 
-						for i in range(len_dataloader):
-
-							Msx = Msx_list[i] # required number of labeled source data for per batch to ensure total of Ms 
+						# create iterators:
+						source_unlabeled_iter = iter(dataloader_source_unlabeled)
+						source_labeled_iter = iter(dataloader_source_labeled)
+						target_unlabeled_iter = iter(dataloader_target_unlabeled)
+						target_labeled_iter = iter(dataloader_target_labeled)
+												
+
+						for i in range(K): 
+							
+							# required number of labeled data for i'th batch to ensure the total of Ms or Mt 
+							Msx = Msx_list[i] 
 							Mtx = Mtx_list[i]
 
-							p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader
+							p = float(i + epoch * K) / n_epoch / K
 							alpha = 2. / (1. + np.exp(-10 * p)) - 1
 
-							# training model using source data
-							data_source = data_source_iter.next()
-							s_img, s_label = data_source
-
 							my_net.zero_grad()
-							batch_size = len(s_label)
+
+							# training model using source data
+							
+							source_unlabeled_img, _ = source_unlabeled_iter.next()
+							if Msx:
+								source_labeled_img, source_labeled_label  = source_labeled_iter.next()
+								for i in range(Msx-1):
+									source_labeled_img_temp, source_labeled_label_temp  = source_labeled_iter.next()
+									source_labeled_img = torch.cat((source_labeled_img,source_labeled_img_temp),dim = 0)
+									source_labeled_label = torch.cat((source_labeled_label,source_labeled_label_temp),dim = 0)
+
+								s_img = torch.cat((source_unlabeled_img[:-Msx,:,:,:],source_labeled_img[:Msx,:,:,:]),dim = 0)
+								s_label = source_labeled_label
+							else:
+								s_img = source_unlabeled_img
+								s_label = torch.rand([0]).long()
+							
+							
 
 							domain_label = torch.zeros(batch_size).long()
+							
+							# batch_size = len(s_label)
 
 							if cuda:
 								s_img = s_img.cuda()
@@ -200,14 +261,25 @@ if __name__ == '__main__':
 
 
 							class_output, domain_output = my_net(input_data=s_img, alpha=alpha)
-							err_s_label = loss_class(class_output[:Msx,:], s_label[:Msx])
+							err_s_label = loss_class(class_output[(batch_size-Msx):,:], s_label)
 							err_s_domain = loss_domain(domain_output, domain_label)
 
 							# training model using target data
-							data_target = data_target_iter.next()
-							t_img, t_label = data_target
-
-							batch_size = len(t_img)
+							target_unlabeled_img, _ = target_unlabeled_iter.next()
+							if Mtx:
+								target_labeled_img, target_labeled_label  = target_labeled_iter.next()
+								for i in range(Mtx-1):
+									target_labeled_img_temp, target_labeled_label_temp  = target_labeled_iter.next()
+									target_labeled_img = torch.cat((target_labeled_img,target_labeled_img_temp),dim = 0)
+									target_labeled_label = torch.cat((target_labeled_label,target_labeled_label_temp),dim = 0)
+
+								t_img = torch.cat((target_unlabeled_img[:-Mtx,:,:,:],target_labeled_img[:Mtx,:,:,:]),dim = 0)
+								t_label = target_labeled_label
+							else:
+								t_img = target_unlabeled_img
+								t_label = torch.rand([0]).long()
+
+							# batch_size = len(t_img)
 
 							domain_label = torch.ones(batch_size).long()
 
@@ -217,8 +289,9 @@ if __name__ == '__main__':
 								domain_label = domain_label.cuda()
 
 							class_output, domain_output = my_net(input_data=t_img, alpha=alpha)
-							err_t_label = loss_class(class_output[:Mtx,:], t_label[:Mtx])
+							err_t_label = loss_class(class_output[(batch_size-Mtx):,:], t_label)
 							err_t_domain = loss_domain(domain_output, domain_label)
+
 							err =(1-beta)*(err_t_domain + err_s_domain) + beta*(err_s_label + err_t_label)
 							err.backward()
 							optimizer.step()
diff --git a/model.py b/model.py
index 9ce6104..9eaf204 100644
--- a/model.py
+++ b/model.py
@@ -9,7 +9,7 @@ class CNNModel(nn.Module):
     def __init__(self,layers, dimchange_multiplier,m_or_n_change):
         super(CNNModel, self).__init__()
 
-        dim_conv = int(64 * np.sqrt(dimchange_multiplier))
+        dim_conv = int(64 * dimchange_multiplier)
         # dim_conv = 64
         # ks = int( 5 *dimchange_multiplier)
         # padding_conv1 = int((ks-5)/2)
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 51cd850..fe0782f 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240319_202843-79p9212c/logs/debug-internal.log
\ No newline at end of file
+run-20240404_223248-6y0ytqu3/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index c876bfe..6dffef5 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240319_202843-79p9212c/logs/debug.log
\ No newline at end of file
+run-20240404_223248-6y0ytqu3/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index d3b180b..8a2ee7d 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240319_202843-79p9212c
\ No newline at end of file
+run-20240404_223248-6y0ytqu3
\ No newline at end of file
